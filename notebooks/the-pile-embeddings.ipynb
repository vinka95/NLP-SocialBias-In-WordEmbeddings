{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfbc6dd-2244-4f67-9b91-7ced1395c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install all required packages, run this cell (can be left out otherwise)\n",
    "!pip install pandas zstandard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954a592-e41d-4672-a4ea-dd1a1f344901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from os import path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, lit, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c7c88-96b8-4290-8fcf-e9d48535d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a spark session and getting the context\n",
    "spark = SparkSession.builder.master(\"yarn\").appName(\"the-pile-embeddings\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642cdb7-3420-44da-b6b8-3b01b39c0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data set paths\n",
    "THE_PILE_BASE_PATH = path.join(\"file:///\", \"mnt\", \"ceph\", \"storage\", \"corpora\", \"corpora-thirdparty\", \"the-pile\")\n",
    "val_data_path = path.join(THE_PILE_BASE_PATH, \"val.jsonl.zst\")\n",
    "test_data_path = path.join(THE_PILE_BASE_PATH, \"test.jsonl.zst\")\n",
    "train_data_paths = [path.join(THE_PILE_BASE_PATH, \"train\", f\"{str(n).zfill(2)}.jsonl.zst\") for n in range(0, 30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5ec50-3721-43da-99ea-728f4fecdc9a",
   "metadata": {},
   "source": [
    "## Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915aa7ba-6797-4913-a893-d90488b9a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set selection\n",
    "data_selection = [\n",
    "    'OpenWebText2',\n",
    "    'PubMed Abstracts',\n",
    "    'StackExchange',\n",
    "    # 'Github', # Currently ignoring because we don't want the code\n",
    "    'Enron Emails',\n",
    "    'FreeLaw',\n",
    "    'USPTO Backgrounds',\n",
    "    'Pile-CC',\n",
    "    'Wikipedia (en)',\n",
    "    'Books3',\n",
    "    'PubMed Central',\n",
    "    'HackerNews',\n",
    "    'Gutenberg (PG-19)',\n",
    "    # 'DM Mathematics', # Currently ignoring because we don't want math formulas\n",
    "    'NIH ExPorter',\n",
    "    'ArXiv',\n",
    "    'BookCorpus2',\n",
    "    'OpenSubtitles',\n",
    "    'YoutubeSubtitles',\n",
    "    'Ubuntu IRC',\n",
    "    # 'EuroParl', # Currently ignoring because we'll focus on English text for now\n",
    "    'PhilPapers'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d4776-e292-41d5-a611-711c5ad24c07",
   "metadata": {},
   "source": [
    "_For now, we use pandas to read the data, as there seems to be some issues with spark reading zstandard compressed files (which \"The Pile\" uses)._\n",
    "\n",
    "_That means, we can also just load parts of the data for now, until we get this issue fixed (probably a server-side issue)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f2a1a-1c47-4cb0-9037-a3dd827fb6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "val_data = pd.read_json(val_data_path, lines=True, compression=\"zstd\")\n",
    "# Transform set name column to make it easier to work with\n",
    "val_data[\"meta_str\"] = val_data[\"meta\"].apply(lambda x: x[\"pile_set_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be9305-bbf1-41a1-a4ae-1410c900bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select the data we are interested in for now\n",
    "filtered_data = val_data[val_data[\"meta_str\"].isin(data_selection)][[\"text\", \"meta_str\"]]\n",
    "\n",
    "# Create a spark dataframe from pandas DataFrame\n",
    "val_data_spark = spark.createDataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db3acb4-8908-477e-9899-8bfd04996a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d1f3c-b36a-4fce-9dec-67eac270faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, we can also transform our data into a dataframe of sentences\n",
    "# First, we split each document into a list of sentences, which are lists of tokens.\n",
    "nested_sentences = val_data_spark.select(\n",
    "    sentences(string=val_data_spark.text, language=lit(\"en\")))\n",
    "\n",
    "# Afterwards, we can flatten the list of lists by \"exploding\" each outer list\n",
    "flattened_sentences = nested_sentences.select(explode(col(\"sentences(text, en, )\")).alias(\"sents\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
