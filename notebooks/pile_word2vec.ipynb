{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb9e3e-5c14-45f3-aa31-46395f1863dd",
   "metadata": {},
   "outputs": [],
   "source": [
    " # To install all required packages, run this cell (can be left out otherwise)\n",
    "!pip install pandas==1.4.0 zstandard gensim sparknlp pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4f4349-b757-43b1-8805-042b1103faa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import json\n",
    "import gc\n",
    "import tempfile\n",
    "\n",
    "from wefe.metrics import RNSB, WEAT\n",
    "from wefe.query import Query\n",
    "from wefe.word_embedding_model import WordEmbeddingModel\n",
    "\n",
    "\n",
    "from os import path\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, explode, lit, sentences\n",
    "from datasets import load_dataset\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872667e2-ecef-4b4e-9285-8e8c1c335689",
   "metadata": {},
   "source": [
    "## Parsing input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02407f02-9140-4ffc-acf7-3691621135e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input_json(file_path):\n",
    "    \"\"\"Method to parse a given input file.\n",
    "\n",
    "    Keyword arguments:\n",
    "    file_path -- path to file containing the input parameters\n",
    "\n",
    "    Return:\n",
    "    data -- parsed data from the input file\n",
    "    \"\"\"\n",
    "    print(file_path)\n",
    "    # Opening JSON file\n",
    "    f = open(file_path)\n",
    "    \n",
    "    print(f)\n",
    "\n",
    "    # returns JSON object as a dictionary\n",
    "    data = json.load(f)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return data\n",
    "\n",
    "def fetch_queries(input_queries):\n",
    "    \"\"\"Method to fetch queries from data/raw/queries.json file.\n",
    "    \"\"\"\n",
    "    query_sets = []\n",
    "    for input_query in input_queries:\n",
    "        query = {}\n",
    "        if 'queries_path_name' in input_query:\n",
    "            # When path to a queries file is given\n",
    "\n",
    "            # declaring local variables for target_sets and attribute_sets\n",
    "            # query = {}\n",
    "            target_sets = []\n",
    "            attribute_sets = []\n",
    "\n",
    "            # Opening JSON file\n",
    "            query_file = open(input_query['queries_path_name'])\n",
    "\n",
    "            # returns queries JSON object as a dictionary\n",
    "            raw_query_data = json.load(query_file)\n",
    "            \n",
    "            target_sets.append(raw_query_data['target_sets'][input_query['target_sets_names'][0]][input_query['target_sets_names'][1]]['set'])\n",
    "            target_sets.append(raw_query_data['target_sets'][input_query['target_sets_names'][0]][input_query['target_sets_names'][2]]['set'])\n",
    "            \n",
    "            # print(raw_query_data['attribute_sets'][input_query['attribute_sets_names'][0]]['set'])\n",
    "            attribute_sets.append(raw_query_data['attribute_sets'][input_query['attribute_sets_names'][0]]['set'])\n",
    "            attribute_sets.append(raw_query_data['attribute_sets'][input_query['attribute_sets_names'][1]]['set'])\n",
    "\n",
    "            # fills up the query dictionary\n",
    "            query['target_sets'] = target_sets\n",
    "            query['attribute_sets'] = attribute_sets\n",
    "            query['target_sets_names'] = [input_query['target_sets_names'][1], input_query['target_sets_names'][2]]\n",
    "            query['attribute_sets_names'] = input_query['attribute_sets_names']\n",
    "\n",
    "            # Closing file\n",
    "            query_file.close()\n",
    "\n",
    "        else:\n",
    "            # when query target and attribute sets are explicitly given\n",
    "            query['target_sets'] = input_query['target_sets']\n",
    "            query['attribute_sets'] = input_query['attribute_sets']\n",
    "            query['target_sets_names'] = input_query['target_sets_names']\n",
    "            query['attribute_sets_names'] = input_query['attribute_sets_names']\n",
    "                                             \n",
    "        query_sets.append(query)\n",
    "\n",
    "    return query_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b3bf4d-479d-46c1-8b57-a44f4380affc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1343c5a-6143-4a9d-85a4-5b57c9f5692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/00.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/01.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/02.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/03.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/04.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/05.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/06.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/07.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/08.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/09.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/10.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/11.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/12.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/13.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/14.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/15.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/16.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/17.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/18.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/19.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/20.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/21.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/22.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/23.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/24.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/25.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/26.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/27.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/28.jsonl.zst', 'file:///mnt/ceph/storage/corpora/corpora-thirdparty/the-pile/train/29.jsonl.zst']\n"
     ]
    }
   ],
   "source": [
    " # Define data set paths\n",
    "THE_PILE_BASE_PATH = path.join(\"file:///\", \"mnt\", \"ceph\", \"storage\", \"corpora\", \"corpora-thirdparty\", \"the-pile\")\n",
    "val_data_path = path.join(THE_PILE_BASE_PATH, \"val.jsonl.zst\")\n",
    "test_data_path = path.join(THE_PILE_BASE_PATH, \"test.jsonl.zst\")\n",
    "train_data_paths = [path.join(THE_PILE_BASE_PATH, \"train\", f\"{str(n).zfill(2)}.jsonl.zst\") for n in range(0, 30)]\n",
    "\n",
    "print(train_data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd0d86e0-04d5-4e02-be4c-0481f57cf859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set selection\n",
    "data_selection = [\n",
    "    'OpenWebText2'\n",
    "    'PubMed Abstracts',\n",
    "    'StackExchange',\n",
    "    # 'Github', # Currently ignoring because we don't want the code\n",
    "    'Enron Emails',\n",
    "    'FreeLaw',\n",
    "    'USPTO Backgrounds',\n",
    "    'Pile-CC',\n",
    "    'Wikipedia (en)',\n",
    "    'Books3',\n",
    "    'PubMed Central',\n",
    "    'HackerNews',\n",
    "    'Gutenberg (PG-19)'\n",
    "    # 'DM Mathematics', # Currently ignoring because we don't want math formulas\n",
    "    'NIH ExPorter',\n",
    "    'ArXiv',\n",
    "    'BookCorpus2',\n",
    "    'OpenSubtitles',\n",
    "    'YoutubeSubtitles',\n",
    "    'Ubuntu IRC',\n",
    "    # 'EuroParl', # Currently ignoring because we'll focus on English text for now\n",
    "    'PhilPapers'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfc54b5-26bd-439f-9512-501eee10ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable declarations\n",
    "model_flag = False\n",
    "model_filepath = \"../models/pile/gensim_word2vec_pile\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4aaa74-d511-4391-99b2-53686c18570d",
   "metadata": {},
   "source": [
    "## Word embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef836052-0a8a-408f-aad0-d2f15ad2dc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_model(text, workers):\n",
    "    \"\"\"Instantiates a word2vec model and builds vocabulary for the model.\n",
    "\n",
    "    Keyword arguments:\n",
    "    text -- dataset, set of tokenized sentences\n",
    "    workers -- no of workers used to train word2vec model\n",
    "\n",
    "    Return:\n",
    "    model -- instance of a word2vec model\n",
    "    \"\"\"\n",
    "    # Instantiates gensim implementation of a Word2Vec model\n",
    "    model = gensim.models.Word2Vec(workers = workers)\n",
    "\n",
    "    # builds vocabulary for the model\n",
    "    print(\"\\nBuilding vocabulary\")\n",
    "    model.build_vocab(text, progress_per=10000)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb422f-29ad-4a40-bd74-d9322aa4880c",
   "metadata": {},
   "source": [
    "## Metrics evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce3e6f71-47e2-4f02-9e49-a623088331b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, query, model_name):\n",
    "    new_query = Query(target_sets = query_data[0]['target_sets'],\n",
    "                               attribute_sets = query_data[0]['attribute_sets'],\n",
    "                               target_sets_names = query_data[0]['target_sets_names'],\n",
    "                               attribute_sets_names = query_data[0]['attribute_sets_names']\n",
    "                               )\n",
    "\n",
    "\n",
    "    we_model = WordEmbeddingModel(model.wv, model_name)\n",
    "    \n",
    "    weat_res = WEAT().run_query(new_query, we_model, lost_vocabulary_threshold=0.5, calculate_p_value=True, p_value_iterations=15000)\n",
    "    \n",
    "    print(we_model.name, weat_res['query_name'], 'WEAT', weat_res['weat'])\n",
    "    \n",
    "    rnsb_res = RNSB().run_query(new_query, we_model, calculate_p_value=True, lost_vocabulary_threshold=0.5, p_value_iterations=15000)\n",
    "    \n",
    "    print(we_model.name, rnsb_res['query_name'], 'RNSB', rnsb_res['rnsb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1cb2f3-6c6a-4fcf-8c63-47a0cd054a34",
   "metadata": {},
   "source": [
    "## Saving models, loading models and free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b3858f-3015-46bc-9646-be4d1083acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    model_filepath = \"../models/pile/gensim_word2vec_pile\" \n",
    "    model.save(model_filepath)\n",
    "    print(\"Model Saved\")\n",
    "    save_model_flag = True\n",
    "    \n",
    "    return save_model_flag\n",
    "\n",
    "def load_model(model_filepath):\n",
    "    print(\"Loading model from : \" + model_filepath)\n",
    "    new_model = gensim.models.Word2Vec.load(model_filepath)\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "def clear_variables():\n",
    "    del data_chunk, filtered_data, df_text, processed_text, model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc30c6f-d3ff-4f2b-9fcf-f5bfa8d048ad",
   "metadata": {},
   "source": [
    "## Training in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9c79c21-d53e-429a-875d-bab3db19f7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/experiments/case1_new_query.json\n",
      "<_io.TextIOWrapper name='../data/experiments/case1_new_query.json' mode='r' encoding='UTF-8'>\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/queries.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d9f771ba5807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Fetching queries from input file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mquery_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_queries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'queries'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Iterating through training paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b9c4929e5975>\u001b[0m in \u001b[0;36mfetch_queries\u001b[0;34m(input_queries)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# Opening JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mquery_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_query\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'queries_path_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# returns queries JSON object as a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/queries.json'"
     ]
    }
   ],
   "source": [
    "# Train word embeddings using Pile data in batches\n",
    "\n",
    "# add start time variable\n",
    "\n",
    "# Parse input file\n",
    "input_data = parse_input_json(\"../data/experiments/case1_new_query.json\")\n",
    "\n",
    "# Fetching queries from input file\n",
    "query_data = fetch_queries(input_data['queries'])\n",
    "\n",
    "# Iterating through training paths\n",
    "for file_path in train_data_paths:\n",
    "    print(file_path)\n",
    "    # Read 'zstd' compressed file and iterate through the file in batches of given chunksize \n",
    "    with pd.read_json(file_path, lines=True, chunksize=1000000, compression='zstd') as reader: \n",
    "        reader\n",
    "        for data_chunk in reader:\n",
    "            # Transform set name column to make it easier to work with\n",
    "            data_chunk[\"meta_str\"] = data_chunk[\"meta\"].apply(lambda x: x[\"pile_set_name\"])\n",
    "            print(len(data_chunk))\n",
    "        \n",
    "            # Only select the data we are interested in for now\n",
    "            filtered_data = data_chunk[data_chunk[\"meta_str\"].isin(data_selection)][[\"text\", \"meta_str\"]]\n",
    "            print(len(filtered_data))\n",
    "            \n",
    "            # Convert filtered data to pandas dataframe for easier processing\n",
    "            df_text = pd.DataFrame(filtered_data['text']).reset_index()\n",
    "\n",
    "            # Apply simple preprocessing(tokenization, removal of punctuations and special characters) on text\n",
    "            # measure how much time?? How to make apply function on multiple cores?\n",
    "            processed_text = df_text['text'].apply(simple_preprocess)\n",
    "            \n",
    "            if model_flag == False:\n",
    "                # Instantiate word2vec and build vocabulary\n",
    "                model = word2vec_model(processed_text, workers=8)\n",
    "                print(model)\n",
    "            else:\n",
    "                model = load_model(model_filepath)\n",
    "                print(model)\n",
    "                \n",
    "            # Training model\n",
    "            model.train(processed_text, total_examples= model.corpus_count, epochs= model.epochs)\n",
    "            \n",
    "            # Saving model\n",
    "            model_flag = save_model(model)\n",
    "            \n",
    "            # Clear variables and free memory\n",
    "            # clear_variables()\n",
    "            del data_chunk, filtered_data, df_text, processed_text, model\n",
    "            gc.collect()\n",
    "            \n",
    "# Evaluate metrics\n",
    "# evaluate_metrics(word2vec, query_data[0], \"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e044fac-6504-4d10-9872-aaf705265dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc74296-e7e7-4168-9174-095fc2039700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b04a6b-b4cc-4a52-b2dc-1e4c98adfe28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0bc052-f944-4a9f-a08e-13b66056a6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2b39a-f2d2-4b72-bd3c-02083ad3cc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15eb95f4-25b3-4a0c-94bb-10af9830367a",
   "metadata": {},
   "source": [
    "### Ignore following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b0b17-7572-48e8-87e7-61329b85326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_paths[1])\n",
    "\n",
    "with pd.read_json(train_data_paths[1], lines=True, chunksize=1000000, compression='zstd') as reader:\n",
    "    reader\n",
    "    len = 0\n",
    "    for chunk in reader:\n",
    "        len+= 1\n",
    "    print(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1330ea1-7ee6-4521-88ed-ac6583a272eb",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Read data\n",
    "# val_data = pd.read_json(val_data_path, lines=True, compression='zstd')\n",
    "\n",
    "train_data = pd.read_json(train_data_paths[0], lines=True, chunksize=234021, compression='zstd')\n",
    "\n",
    "print(len(train_data))\n",
    "# Transform set name column to make it easier to work with\n",
    "val_data[\"meta_str\"] = val_data[\"meta\"].apply(lambda x: x[\"pile_set_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ebcbf0-737d-43ed-8dd4-ba2bca413cde",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filtering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228f2ab-f660-4c67-86c0-880122754c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Only select the data we are interested in for now\n",
    "filtered_data = val_data[val_data[\"meta_str\"].isin(data_selection)][[\"text\", \"meta_str\"]]\n",
    "\n",
    "text1 = []\n",
    "text1 = filtered_data['text']\n",
    "\n",
    "df_text = pd.DataFrame(text1).reset_index()\n",
    "\n",
    "# Apply simple preprocessing(tokenization, removal of punctuations and special characters) on text\n",
    "proc_text = df_text['text'].apply(simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3ba99-64a1-4948-9289-7f3c9d7542ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e22ab9-d5df-4ee5-bc97-6971b2ae13c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train word embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ebf29-bc5e-40fc-a43d-81aa26d40435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate word2vec and build vocabulary\n",
    "word2vec = word2vec_model(proc_text, workers=8)\n",
    "print(word2vec)\n",
    "\n",
    "# Training model\n",
    "word2vec.train(proc_text, total_examples=word2vec.corpus_count, epochs=word2vec.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c1e34-3ce1-422f-922f-59bf2f6379c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_model(text, workers):\n",
    "    \"\"\"Instantiates a fasttext model and builds vocabulary for the model.\n",
    "\n",
    "    Keyword arguments:\n",
    "    text -- dataset, set of tokenized sentences\n",
    "    workers -- no of workers used to train\n",
    "\n",
    "    Return:\n",
    "    model -- instance of a fasttext model\n",
    "    \"\"\"\n",
    "    # Instantiates gensim implementation of a Fasttext model\n",
    "    model = gensim.models.FastText()\n",
    "    \n",
    "    print(\"\\nBuilding vocabulary\")\n",
    "    # build the vocabulary\n",
    "    model.build_vocab(corpus_file=corpus_file)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34104e-578f-4e2f-8ab5-395475a35bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate fasttext and build vocabulary\n",
    "fasttext = fasttext_model(proc_text, workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa3317-8a93-4869-98a9-5dcd2816a86a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parsing input query and metrics evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a65f3-d1a7-4329-92c9-9dcdd25191a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(model, query, model_name):\n",
    "    new_query = Query(target_sets = query_data[0]['target_sets'],\n",
    "                               attribute_sets = query_data[0]['attribute_sets'],\n",
    "                               target_sets_names = query_data[0]['target_sets_names'],\n",
    "                               attribute_sets_names = query_data[0]['attribute_sets_names']\n",
    "                               )\n",
    "\n",
    "\n",
    "    we_model = WordEmbeddingModel(model.wv, model_name)\n",
    "    \n",
    "    weat_res = WEAT().run_query(new_query, we_model, lost_vocabulary_threshold=0.5, calculate_p_value=True, p_value_iterations=15000)\n",
    "    \n",
    "    print(we_model.name, weat_res['query_name'], 'WEAT', weat_res['weat'])\n",
    "    \n",
    "    rnsb_res = RNSB().run_query(new_query, we_model, calculate_p_value=True, lost_vocabulary_threshold=0.5, p_value_iterations=15000)\n",
    "    \n",
    "    print(we_model.name, rnsb_res['query_name'], 'RNSB', rnsb_res['rnsb'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e54297-a18d-49b2-bb1b-0efe6a109f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input_json(file_path):\n",
    "    \"\"\"Method to parse a given input file.\n",
    "\n",
    "    Keyword arguments:\n",
    "    file_path -- path to file containing the input parameters\n",
    "\n",
    "    Return:\n",
    "    data -- parsed data from the input file\n",
    "    \"\"\"\n",
    "    print(file_path)\n",
    "    # Opening JSON file\n",
    "    f = open(file_path)\n",
    "    \n",
    "    print(f)\n",
    "\n",
    "    # returns JSON object as a dictionary\n",
    "    data = json.load(f)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return data\n",
    "\n",
    "def fetch_queries(input_queries):\n",
    "    \"\"\"Method to fetch queries from data/raw/queries.json file.\n",
    "    \"\"\"\n",
    "    query_sets = []\n",
    "    for input_query in input_queries:\n",
    "        query = {}\n",
    "        if 'queries_path_name' in input_query:\n",
    "            # When path to a queries file is given\n",
    "\n",
    "            # declaring local variables for target_sets and attribute_sets\n",
    "            # query = {}\n",
    "            target_sets = []\n",
    "            attribute_sets = []\n",
    "\n",
    "            # Opening JSON file\n",
    "            query_file = open(input_query['queries_path_name'])\n",
    "\n",
    "            # returns queries JSON object as a dictionary\n",
    "            raw_query_data = json.load(query_file)\n",
    "            \n",
    "            target_sets.append(raw_query_data['target_sets'][input_query['target_sets_names'][0]][input_query['target_sets_names'][1]]['set'])\n",
    "            target_sets.append(raw_query_data['target_sets'][input_query['target_sets_names'][0]][input_query['target_sets_names'][2]]['set'])\n",
    "            \n",
    "            # print(raw_query_data['attribute_sets'][input_query['attribute_sets_names'][0]]['set'])\n",
    "            attribute_sets.append(raw_query_data['attribute_sets'][input_query['attribute_sets_names'][0]]['set'])\n",
    "            attribute_sets.append(raw_query_data['attribute_sets'][input_query['attribute_sets_names'][1]]['set'])\n",
    "\n",
    "            # fills up the query dictionary\n",
    "            query['target_sets'] = target_sets\n",
    "            query['attribute_sets'] = attribute_sets\n",
    "            query['target_sets_names'] = [input_query['target_sets_names'][1], input_query['target_sets_names'][2]]\n",
    "            query['attribute_sets_names'] = input_query['attribute_sets_names']\n",
    "\n",
    "            # Closing file\n",
    "            query_file.close()\n",
    "\n",
    "        else:\n",
    "            # when query target and attribute sets are explicitly given\n",
    "            query['target_sets'] = input_query['target_sets']\n",
    "            query['attribute_sets'] = input_query['attribute_sets']\n",
    "            query['target_sets_names'] = input_query['target_sets_names']\n",
    "            query['attribute_sets_names'] = input_query['attribute_sets_names']\n",
    "                                             \n",
    "        query_sets.append(query)\n",
    "\n",
    "    return query_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d93fda-94ca-4cfd-a036-8404d1e62155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse input file\n",
    "input_data = parse_input_json(\"../data/experiments/case1_new_query.json\")\n",
    "\n",
    "# Fetching queries\n",
    "query_data = fetch_queries(input_data['queries'])\n",
    "\n",
    "# Evaluate metrics\n",
    "evaluate_metrics(word2vec, query_data[0], \"word2vec\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
